{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev_sample_percetage': 0.1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import data_helpers\n",
    "import numpy as np\n",
    "\n",
    "tf.flags.DEFINE_float('dev_sample_percetage',0.1,'Percetafe of training data to use for vaildation')\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.flag_values_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a19ae32d69f1>:4: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Snake\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Snake\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "file_path = r'F:\\post graduate\\raw_data\\bugs.csv'\n",
    "x_text,y = data_helpers.load_data_and_labels(file_path)\n",
    "max_document_length = max(len(x.split(' ')) for x in x_text)\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length,min_frequency=2)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print(max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,  34, ...,   0,   0,   0],\n",
       "       [  1,   2,  34, ...,   0,   0,   0],\n",
       "       [  1,   2,  34, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  1,  13,  14, ...,   0,   0,   0],\n",
       "       [ 26,  25, 151, ...,   0,   0,   0],\n",
       "       [  1,   2, 426, ...,   0,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_indices = np.random.permutation(np.arange(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shuffled = x[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  13,  14, ...,   0,   0,   0],\n",
       "       [  1,   2,  16, ...,   0,   0,   0],\n",
       "       [  1,   4,   3, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [128, 128, 404, ...,   0,   0,   0],\n",
       "       [  1,   2,  19, ...,   0,   0,   0],\n",
       "       [  1,   4,  27, ...,   0,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9198b2acf60a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdev_sample_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdev_sample_percentage\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# a flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m       \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, argv, known_only)\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[1;32m--> 633\u001b[1;33m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12525"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_sample_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62625\n"
     ]
    }
   ],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes= list(map(int, '3,4,5'.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = np.zeros((10, 10), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(one_hot, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "vocab = []\n",
    "embd = []\n",
    "emb_size = 200\n",
    "filename = r'F:\\word2vec\\glove.6B\\glove.6B.200d.txt'\n",
    "vocab.append('unk')  # 装载不认识的词\n",
    "embd.append([0] * emb_size)  # 这个emb_size可能需要指定\n",
    "file = open(filename, 'r', encoding='utf-8')\n",
    "for line in file.readlines():\n",
    "    row = line.strip().split(' ')\n",
    "    vocab.append(row[0])\n",
    "    embd.append(row[1:])\n",
    "print('Loaded GloVe!')\n",
    "file.close()\n",
    "with open(r'F:\\bug_triage\\emb_and_dict\\dict200.pickle', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "with open(r'F:\\bug_triage\\emb_and_dict\\embedding200.pickle', 'wb') as f:\n",
    "    pickle.dump(embd, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "stop_word = stopwords.words(\"english\")\n",
    "def load_data_and_labels(data_file):\n",
    "    df = pd.read_csv(data_file, encoding='latin-1')\n",
    "    text = df['x_input']\n",
    "    label = df['y_label']\n",
    "    x_text = [clean_str(sent) for sent in text]\n",
    "    \n",
    "    set_label = set(label)\n",
    "    length = len(set_label)\n",
    "    dic_label = dict(zip(range(len(set_label)), set_label))\n",
    "    y = list()\n",
    "    for name in label:\n",
    "        l = [0] * length\n",
    "        for k, v in dic_label.items():\n",
    "            if name is v:\n",
    "                l[k] = 1\n",
    "                y.append(l)\n",
    "    return [x_text, np.asarray(y)]\n",
    "def clean_str(string):\n",
    "    porter=nltk.PorterStemmer()\n",
    "    string = re.sub(r\"\\(.*?\\)\", \" \", string)\n",
    "    string = re.sub(r'[^a-zA-Z \\']', \" \", string)\n",
    "    string = [porter.stem(w) for w in string.split() if w not in stop_word ]\n",
    "    string = \" \".join(string)\n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_word)\n",
    "x,y = load_data_and_labels(r'F:\\post graduate\\dataset\\filted_data.csv')\n",
    "clean_str(\"Tools CDT cdt-core Main thread hangs due to deadlock whilst indexing\")\n",
    "porter=nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-55fd0d44328b>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Snake\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(100, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open(r'F:\\bug_triage\\emb_and_dict\\embedding100.pickle', 'rb') as f:\n",
    "        emb = pickle.load(f)\n",
    "        emb = np.asarray(emb)\n",
    "        emb = emb.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'F:\\bug_triage\\emb_and_dict\\dict100.pickle', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation:\n",
      "2019-05-22 15:09:10.030217: step 12400, loss 1.85833, acc 0.553313\n",
      "2019-05-22T15:09:10.127956: step 12401, loss 1.13926, acc 0.78125\n",
      "2019-05-22T15:09:10.242649: step 12402, loss 1.21519, acc 0.671875\n",
      "2019-05-22T15:09:10.350361: step 12403, loss 1.41069, acc 0.609375\n",
      "2019-05-22T15:09:10.459071: step 12404, loss 1.15076, acc 0.6875\n",
      "2019-05-22T15:09:10.576756: step 12405, loss 1.26219, acc 0.59375\n",
      "2019-05-22T15:09:10.683470: step 12406, loss 1.13599, acc 0.6875\n",
      "2019-05-22T15:09:10.793176: step 12407, loss 0.838597, acc 0.828125\n",
      "2019-05-22T15:09:10.899891: step 12408, loss 1.26273, acc 0.65625\n",
      "2019-05-22T15:09:11.012589: step 12409, loss 0.907215, acc 0.8125\n",
      "2019-05-22T15:09:11.127282: step 12410, loss 1.21336, acc 0.671875\n",
      "2019-05-22T15:09:11.241976: step 12411, loss 1.19814, acc 0.65625\n",
      "2019-05-22T15:09:11.355672: step 12412, loss 1.42266, acc 0.65625\n",
      "2019-05-22T15:09:11.467373: step 12413, loss 1.17634, acc 0.65625\n",
      "2019-05-22T15:09:11.579074: step 12414, loss 1.3206, acc 0.71875\n",
      "2019-05-22T15:09:11.690776: step 12415, loss 1.23893, acc 0.671875\n",
      "2019-05-22T15:09:11.799484: step 12416, loss 1.32823, acc 0.609375\n",
      "2019-05-22T15:09:11.910188: step 12417, loss 1.05198, acc 0.71875\n",
      "2019-05-22T15:09:12.020892: step 12418, loss 1.34072, acc 0.625\n",
      "2019-05-22T15:09:12.132593: step 12419, loss 1.1722, acc 0.671875\n"
     ]
    }
   ],
   "source": [
    "print(\" Evaluation:\\n\\\n",
    "2019-05-22 15:09:10.030217: step 12400, loss 1.85833, acc 0.553313\\n\\\n",
    "2019-05-22T15:09:10.127956: step 12401, loss 1.13926, acc 0.78125\\n\\\n",
    "2019-05-22T15:09:10.242649: step 12402, loss 1.21519, acc 0.671875\\n\\\n",
    "2019-05-22T15:09:10.350361: step 12403, loss 1.41069, acc 0.609375\\n\\\n",
    "2019-05-22T15:09:10.459071: step 12404, loss 1.15076, acc 0.6875\\n\\\n",
    "2019-05-22T15:09:10.576756: step 12405, loss 1.26219, acc 0.59375\\n\\\n",
    "2019-05-22T15:09:10.683470: step 12406, loss 1.13599, acc 0.6875\\n\\\n",
    "2019-05-22T15:09:10.793176: step 12407, loss 0.838597, acc 0.828125\\n\\\n",
    "2019-05-22T15:09:10.899891: step 12408, loss 1.26273, acc 0.65625\\n\\\n",
    "2019-05-22T15:09:11.012589: step 12409, loss 0.907215, acc 0.8125\\n\\\n",
    "2019-05-22T15:09:11.127282: step 12410, loss 1.21336, acc 0.671875\\n\\\n",
    "2019-05-22T15:09:11.241976: step 12411, loss 1.19814, acc 0.65625\\n\\\n",
    "2019-05-22T15:09:11.355672: step 12412, loss 1.42266, acc 0.65625\\n\\\n",
    "2019-05-22T15:09:11.467373: step 12413, loss 1.17634, acc 0.65625\\n\\\n",
    "2019-05-22T15:09:11.579074: step 12414, loss 1.3206, acc 0.71875\\n\\\n",
    "2019-05-22T15:09:11.690776: step 12415, loss 1.23893, acc 0.671875\\n\\\n",
    "2019-05-22T15:09:11.799484: step 12416, loss 1.32823, acc 0.609375\\n\\\n",
    "2019-05-22T15:09:11.910188: step 12417, loss 1.05198, acc 0.71875\\n\\\n",
    "2019-05-22T15:09:12.020892: step 12418, loss 1.34072, acc 0.625\\n\\\n",
    "2019-05-22T15:09:12.132593: step 12419, loss 1.1722, acc 0.671875\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
